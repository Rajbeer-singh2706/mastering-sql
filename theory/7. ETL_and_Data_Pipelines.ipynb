{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. ETL and Data Pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Q) How would you implement an ETL process in SQL? What are the challenges in handling large data transformations, and how would you optimize them?\n",
    "Q) Explain how you would design a data pipeline to handle incremental loads (only new or changed data) using SQL.\n",
    "Q) What are the best practices for maintaining historical data in a data warehouse? How would you structure your tables to accommodate time-series data?\n",
    "\n",
    "Give answer of these in details with example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q1) How would you implement an ETL process in SQL? What are the challenges in handling large data transformations, and how would you optimize them?**  \n",
    "\n",
    "#### **ETL (Extract, Transform, Load) in SQL**\n",
    "An **ETL (Extract, Transform, Load)** process in SQL typically follows three key steps:\n",
    "\n",
    "1. **Extract:** Pulling data from different sources such as databases, APIs, flat files, or external systems.  \n",
    "2. **Transform:** Cleaning, filtering, aggregating, and modifying the data to meet business requirements.  \n",
    "3. **Load:** Storing the transformed data into a target database or data warehouse.\n",
    "\n",
    "\n",
    "### **SQL-Based ETL Process**\n",
    "#### **Step 1: Extract Data from Multiple Sources**\n",
    "```sql\n",
    "SELECT * FROM staging.sales_data;\n",
    "SELECT * FROM staging.customer_data;\n",
    "```\n",
    "- Data can be pulled from different databases, CSV files, or APIs.  \n",
    "- In some cases, **ETL tools like Apache NiFi, Talend, or SQL Server Integration Services (SSIS)** are used to extract data.\n",
    "\n",
    "#### **Step 2: Transform Data**\n",
    "Common transformations include:\n",
    "- **Data Cleaning:** Handling NULL values, removing duplicates.\n",
    "- **Data Aggregation:** Summarizing records using `GROUP BY`.\n",
    "- **Data Type Conversions:** Ensuring consistent data formats.\n",
    "\n",
    "##### **Example: Transforming Sales Data**\n",
    "```sql\n",
    "INSERT INTO transformed.sales_summary (customer_id, total_amount, order_count, last_purchase_date)\n",
    "SELECT \n",
    "    customer_id, \n",
    "    SUM(sales_amount) AS total_amount, \n",
    "    COUNT(order_id) AS order_count, \n",
    "    MAX(order_date) AS last_purchase_date\n",
    "FROM staging.sales_data\n",
    "GROUP BY customer_id;\n",
    "```\n",
    "\n",
    "#### **Step 3: Load Data into the Target Warehouse**\n",
    "```sql\n",
    "INSERT INTO data_warehouse.sales_summary\n",
    "SELECT * FROM transformed.sales_summary;\n",
    "```\n",
    "- This can be done using **batch processing** or **incremental loading**.  \n",
    "- **Indexes and partitions** should be optimized for better performance.\n",
    "\n",
    "\n",
    "### **Challenges in Handling Large Data Transformations**\n",
    "1. **Performance Bottlenecks:** Large datasets can slow down queries.\n",
    "2. **Data Integrity Issues:** Inconsistent data across multiple sources.\n",
    "3. **Concurrency Issues:** Multiple users running transformations simultaneously.\n",
    "4. **Long ETL Processing Time:** Especially if data volume is in **terabytes**.\n",
    "5. **Failure Recovery:** Partial failures can corrupt data loads.\n",
    "\n",
    "### **Optimizations for Large ETL Processes**\n",
    "âœ… **Use Indexing and Partitioning:** Improves query speed.  \n",
    "âœ… **Incremental Loads Instead of Full Loads:** Avoids redundant data processing.  \n",
    "âœ… **Batch Processing:** Load data in smaller batches instead of one large operation.  \n",
    "âœ… **Parallel Processing:** Distribute workloads across multiple nodes using **distributed databases (e.g., Snowflake, BigQuery)**.  \n",
    "âœ… **Materialized Views & Precomputed Aggregates:** Reduces processing time for repeated queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q2) Explain how you would design a data pipeline to handle incremental loads (only new or changed data) using SQL.**  \n",
    " ðŸ”¹ **Incremental loading** ensures that only **new or modified data** is extracted, transformed, and loaded instead of reprocessing all records.\n",
    "\n",
    "### **Approach 1: Using a `last_update` Timestamp**\n",
    "- Many databases include a **`last_updated`** timestamp column.\n",
    "- You can use this column to fetch only **new or updated records**.\n",
    "\n",
    "#### **Example: Extract Only New/Modified Records**\n",
    "```sql\n",
    "SELECT * FROM staging.sales_data \n",
    "WHERE last_updated > (SELECT MAX(last_updated) FROM data_warehouse.sales_data);\n",
    "```\n",
    "- This extracts records that are newer than the last processed timestamp.  \n",
    "\n",
    "#### **Example: Merging Data into Data Warehouse**\n",
    "```sql\n",
    "MERGE INTO data_warehouse.sales_data AS target\n",
    "USING staging.sales_data AS source\n",
    "    ON target.order_id = source.order_id\n",
    "WHEN MATCHED THEN \n",
    "    UPDATE SET target.sales_amount = source.sales_amount, \n",
    "               target.last_updated = source.last_updated\n",
    "WHEN NOT MATCHED THEN \n",
    "    INSERT (order_id, customer_id, sales_amount, last_updated)\n",
    "    VALUES (source.order_id, source.customer_id, source.sales_amount, source.last_updated);\n",
    "```\n",
    "âœ… **MERGE ensures**:\n",
    "- If the record **exists**, it **updates** it.  \n",
    "- If the record **does not exist**, it **inserts** a new one.  \n",
    "\n",
    "### **Approach 2: Using Change Data Capture (CDC)**\n",
    "- **CDC captures changes** (INSERT, UPDATE, DELETE) in source tables.  \n",
    "- It avoids scanning the entire table and improves performance.  \n",
    "\n",
    "#### **Example: Using CDC in SQL Server**\n",
    "```sql\n",
    "SELECT * FROM cdc.fn_cdc_get_all_changes_dbo_sales_data (@from_lsn, @to_lsn);\n",
    "```\n",
    "- Retrieves **only changed records** from the log sequence numbers (LSN).\n",
    "\n",
    "### **Best Practices for Incremental Loading**\n",
    "âœ… **Maintain an Audit Table:** Store the last processed timestamp.  \n",
    "âœ… **Index the `last_updated` Column:** Helps in filtering new records faster.  \n",
    "âœ… **Use CDC Where Available:** Reduces database load.  \n",
    "âœ… **Partition Large Tables by Date:** Improves query performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q3) What are the best practices for maintaining historical data in a data warehouse? How would you structure your tables to accommodate time-series data?**\n",
    "ðŸ”¹ **Historical data** is important for **trend analysis, reporting, and compliance.**  \n",
    "\n",
    "### **Best Practices for Storing Historical Data**\n",
    "\n",
    "âœ… **Slowly Changing Dimensions (SCDs):**       Track historical changes in reference data.  \n",
    "âœ… **Partitioning Tables by Date:**             Optimizes queries by reducing scanned data.  \n",
    "âœ… **Fact and Dimension Tables:**               Use **star or snowflake schema** for efficient storage.  \n",
    "âœ… **Data Retention Policies:**                 Archive older data to reduce active storage usage.  \n",
    "âœ… **Use Append-Only Strategy:**                Do not overwrite historical records unless necessary.  \n",
    "\n",
    "### **Structuring Tables for Time-Series Data**\n",
    "âœ… **Approach 1: Append New Data Instead of Overwriting**\n",
    "```sql\n",
    "CREATE TABLE sales_history (\n",
    "    order_id INT,\n",
    "    customer_id INT,\n",
    "    sales_amount DECIMAL(10,2),\n",
    "    order_date DATE,\n",
    "    version TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "```\n",
    "- Each update creates a **new version** instead of modifying existing records.  \n",
    "\n",
    "#### **Example: Inserting a New Version When Data Changes**\n",
    "```sql\n",
    "INSERT INTO sales_history (order_id, customer_id, sales_amount, order_date)\n",
    "SELECT order_id, customer_id, sales_amount, order_date\n",
    "FROM staging.sales_data;\n",
    "```\n",
    "- This method ensures **historical tracking** of data changes.\n",
    "\n",
    "### **Slowly Changing Dimensions (SCD)**\n",
    "- Used when **historical tracking of changes in reference data** is required.\n",
    "\n",
    "#### **Approach 2: Slowly Changing Dimension Type 2 (SCD Type 2)**\n",
    "```sql\n",
    "CREATE TABLE customer_history (\n",
    "    customer_id INT,\n",
    "    name VARCHAR(100),\n",
    "    email VARCHAR(100),\n",
    "    address VARCHAR(255),\n",
    "    start_date DATE,\n",
    "    end_date DATE DEFAULT NULL\n",
    ");\n",
    "```\n",
    "âœ… **Whenever a customerâ€™s information is updated, a new record is inserted.**  \n",
    "âœ… **The previous record is marked with an `end_date`.**  \n",
    "\n",
    "#### **Example: Handling Customer Address Change**\n",
    "```sql\n",
    "UPDATE customer_history\n",
    "SET end_date = CURRENT_DATE\n",
    "WHERE customer_id = 101 AND end_date IS NULL;\n",
    "\n",
    "INSERT INTO customer_history (customer_id, name, email, address, start_date)\n",
    "VALUES (101, 'John Doe', 'john@example.com', 'New Address', CURRENT_DATE);\n",
    "```\n",
    "âœ… **This allows us to track changes over time without losing history.**  \n",
    "\n",
    "\n",
    "### **Summary of Key Takeaways**\n",
    "1. **ETL Implementation in SQL:**\n",
    "   - Extract from source tables.\n",
    "   - Transform using filtering, aggregation, and cleaning.\n",
    "   - Load into the data warehouse efficiently.\n",
    "   - Optimize using **partitioning, indexing, and batch processing.**\n",
    "   \n",
    "2. **Incremental Data Loading:**\n",
    "   - Use a **`last_updated`** column.\n",
    "   - Use **MERGE statements** for updates and inserts.\n",
    "   - Utilize **Change Data Capture (CDC)** if available.\n",
    "\n",
    "3. **Historical Data Storage in a Data Warehouse:**\n",
    "   - Use **Slowly Changing Dimensions (SCD Type 2)** to track changes.\n",
    "   - Partition large tables by **time-based columns**.\n",
    "   - Avoid updates; use **append-only models** for better performance.\n",
    "\n",
    "Would you like a deeper dive into any specific topic? ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
